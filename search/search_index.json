{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"eXRercise sub-project of the MASTER first Open Call (OC)","text":"<p>The following documentation addresses the integration of assets developed using Unity Engine and the Unity Editor, for the purposes of the MASTER OC project eXercise.</p> <p>One of the components of the eXercise Training System is the creation of VR-based scenarios. The library described below enables dynamic VR scenarios for: - fire incidents (by creating a virtual fire effect at a robotic arm/device) - malfunction scenarios at a specified robotic arm or device</p> <p>This component accepts structured decisions output from an LLM prompt, used by a trainer. The result is a VR scenario presented to a trainee. It is distributed as a Unity-native DLL library.</p>"},{"location":"huggingface/","title":"Pilot Project Report: NLP-based Trainer","text":""},{"location":"huggingface/#1-objective","title":"1 Objective","text":"<p>The primary aim of this pilot project was to create a streamlined, interactive interface enabling trainers to input high-level textual descriptions (prompts) regarding training scenarios involving virtual fires and robotic arm malfunctions. These inputs are processed by advanced language models (LLMs) to generate scenario recommendations, identify malfunctions, and support training decisions.</p>"},{"location":"huggingface/#2-overview-of-system","title":"2 Overview of System","text":"<p>The system has been developed using Streamlit and comprises two key interface sections:</p> <ol> <li>Robotic Arm Selector Tab  </li> <li>AI Advisor Tab</li> </ol>"},{"location":"huggingface/#21-robotic-arm-selector-tab","title":"2.1 Robotic Arm Selector Tab","text":"<p>This tab provides users (trainers) with the ability to:</p> <ul> <li>Select an LLM model, supporting both OpenAI and Hugging Face APIs (e.g., GPT-4o, Zephyr, Mistral).</li> <li>Dynamically input the number of robotic arms for the session.</li> <li>Submit textual prompts describing training scenarios.</li> <li>Enter distance values for each robotic arm relative to the trainee.</li> </ul> <p>The system then:</p> <ul> <li>Processes the prompt and distances using the selected LLM.</li> <li>Returns structured JSON output indicating:</li> <li>Which robotic arm may be experiencing a virtual fire.</li> <li>Which arm is potentially malfunctioning.</li> <li>An explanation for the AI\u2019s decision.</li> <li>Displays example prompts to guide user input.</li> <li>Persists all session data for later analysis.</li> </ul> <p> Figure 1: Robotic Arm Selector Tab</p>"},{"location":"huggingface/#22-ai-advisor-tab","title":"2.2 AI Advisor Tab","text":"<p>This tab supports two advanced functions:</p> <ol> <li> <p>Session Stress Result Retrieval: Connects to an external stress classification API to retrieve the participant\u2019s emotional/stress performance based on physiological data.</p> </li> <li> <p>Performance Report Generation: Uses the LLM to generate a markdown report incorporating:</p> </li> <li>Session metadata (date, time, participant ID)</li> <li>Robotic arm distances</li> <li>Malfunction and fire detection summary</li> <li>Stress analysis results</li> <li>Analytical tables and conclusions</li> </ol> <p> Figure 2: AI Advisor Tab</p>"},{"location":"huggingface/#3-system-components","title":"3 System Components","text":""},{"location":"huggingface/#31-natural-language-interface","title":"3.1 Natural Language Interface","text":"<p>The platform leverages large language models (via OpenAI and Hugging Face) for prompt processing. The system dynamically constructs a structured prompt containing:</p> <ul> <li>Trainer\u2019s textual input</li> <li>Robotic arm distance values</li> <li>Request for identification of robotic arm associated with virtual fire and malfunction</li> </ul>"},{"location":"huggingface/#32-distance-input-panel","title":"3.2 Distance Input Panel","text":"<p>Users can input precise numerical distances for each robotic arm. These values are used by the LLMs to make scenario inferences.</p>"},{"location":"huggingface/#33-model-integration","title":"3.3 Model Integration","text":"<p>Models supported include:</p> <ul> <li>GPT-4o, GPT-4o-mini (OpenAI API)</li> <li>Zephyr-7b-beta, Mistral-7B-Instruct-v0.3, Qwen2.5-72B-Instruct (Hugging Face Inference API)</li> </ul> <p>The list of models can be extended to include others.</p>"},{"location":"huggingface/#34-stress-analysis","title":"3.4 Stress Analysis","text":"<p>The system queries a stress analysis API to retrieve a participant\u2019s classification result for the session. It matches timestamps and participant IDs to fetch the correct entry.</p>"},{"location":"huggingface/#4-sample-output-format","title":"4 Sample Output Format","text":"<pre><code>{\n  \"Robotic_Arm_Virtual_Fire\": 1,\n  \"Robotic_Arm_Malfunctioning\": 2,\n  \"Reason_of_selection\": \"Robotic Arm Number 1 is the closest to the base with a distance of 1.43, which aligns with the description of the virtual fire being 'close to the base.' Robotic Arm Number 2, at a distance of 7.79, is the farthest and matches the description of the arm 'moving erratically at a far distance,' indicating it is potentially malfunctioning.\"\n}\n</code></pre> <p>Generated with OpenAI API (gpt-4o).</p>"},{"location":"huggingface/#5-usability-and-user-experience","title":"5 Usability and User Experience","text":"<ul> <li>The interface is intuitive and provides real-time feedback.</li> <li>Prompts with fewer than 20 characters are disallowed to prevent vague input.</li> <li>Prompt inputs are stored and reused for report generation.</li> <li>Users can view AI model outputs in both JSON and Chat UI formats.</li> </ul>"},{"location":"huggingface/#6-challenges-and-recommendations","title":"6 Challenges and Recommendations","text":"<ul> <li> <p>Challenge: Occasional JSON parsing failures from LLM responses.   Recommendation: Implement retry logic and more constrained prompt formatting.</p> </li> <li> <p>Challenge: Ambiguity in prompt interpretation when language is imprecise.   Recommendation: Enhance prompt scaffolding and allow iterative refinement.</p> </li> </ul>"},{"location":"huggingface/#7-conclusion","title":"7 Conclusion","text":"<p>The pilot project successfully demonstrated the feasibility of translating natural language trainer inputs into actionable XR training scenarios. The integration of multiple LLMs, combined with real-time data capture and post-session reporting, creates a powerful tool for enhancing safety training simulations.</p>"},{"location":"installation/","title":"Robotic Arm &amp; AI Advisor Installation &amp; Deployment Guide","text":"<p>This guide walks you through deploying the Robotic Arm &amp; AI Advisor Streamlit app to a Hugging Face Space and configuring a Hugging Face dataset <code>XYZ</code> for storing LLM result JSON files.</p>"},{"location":"installation/#1-prerequisites","title":"1. Prerequisites","text":"<p>Make sure you have:</p> <ul> <li>A Hugging Face account</li> <li><code>git</code> installed on your machine</li> <li><code>Python &gt;= 3.11</code> installed</li> <li>A valid Hugging Face token (<code>HF_TOKEN</code>) with write permissions</li> <li>(Optional) An OpenAI API key (<code>OPENAI_API_KEY</code>) if using OpenAI models</li> </ul>"},{"location":"installation/#2-create-a-hugging-face-space","title":"2. Create a Hugging Face Space","text":"<ol> <li>Go to Create a New Space.</li> <li>Fill in:<ul> <li>SDK: <code>Streamlit</code></li> <li>Space Name: e.g., <code>robotic-arm-selector</code></li> <li>License: Select one appropriate</li> <li>Visibility: <code>Public</code> or <code>Private</code></li> </ul> </li> <li>Click Create Space.</li> </ol>"},{"location":"installation/#3-create-and-add-access-tokens","title":"3. Create and Add Access Tokens","text":""},{"location":"installation/#a-create-hugging-face-token-hf_token","title":"A. Create Hugging Face Token (<code>HF_TOKEN</code>)","text":"<ol> <li>Visit Hugging Face Tokens.</li> <li>Click New token, name it <code>HF_TOKEN</code>, set permissions to Write.</li> <li>Copy the generated token securely.</li> </ol>"},{"location":"installation/#b-add-hugging-face-token-to-space","title":"B. Add Hugging Face Token to Space","text":"<ol> <li>In your Space, go to Settings &gt; Secrets.</li> <li>Add a new secret:</li> <li>Name: <code>HF_TOKEN</code></li> <li>Value: your copied token</li> </ol>"},{"location":"installation/#c-optional-add-openai-token","title":"C. (Optional) Add OpenAI Token","text":"<p>If using OpenAI models (<code>gpt-4o</code>, etc.), add:</p> <ul> <li>Name: <code>OPENAI_API_KEY</code></li> <li>Value: your OpenAI API key</li> </ul>"},{"location":"installation/#4-prepare-and-upload-your-files","title":"4. Prepare and Upload Your Files","text":"<p>You need these files in your Space repository:</p>"},{"location":"installation/#roboticarmpy","title":"<code>RoboticArm.py</code>","text":"<p>Your main Streamlit app. Ensure it includes code for uploading LLM result JSON to a Hugging Face dataset (see Section 7). See source content</p>"},{"location":"installation/#requirementstxt","title":"<code>requirements.txt</code>","text":"<p>List dependencies: <pre><code>streamlit\nhuggingface_hub&gt;=0.31.4\nsentence-transformers\nopenai\npydantic\nrequests\n</code></pre></p>"},{"location":"installation/#readmemd","title":"<code>README.md</code>","text":"<p>Configure your Space: <pre><code>title: RoboticArm\nsdk: streamlit\nemoji: (see file to render)\ncolorFrom: pink\ncolorTo: indigo\nsdk_version: 1.44.1\napp_file: RoboticArm.py\npinned: false\npython_version: '3.11'\n</code></pre></p>"},{"location":"installation/#5-upload-and-deploy","title":"5. Upload and Deploy","text":""},{"location":"installation/#option-a-upload-through-web-ui","title":"Option A: Upload through Web UI","text":"<ol> <li>In your Space, go to Files and versions.</li> <li>Click Add file &gt; Upload files.</li> <li>Upload <code>RoboticArm.py</code>, <code>README.md</code>, and <code>requirements.txt</code>.</li> </ol>"},{"location":"installation/#option-b-upload-via-git","title":"Option B: Upload via Git","text":"<pre><code># Clone your space\ngit clone https://huggingface.co/spaces/&lt;your-username&gt;/robotic-arm-selector\ncd robotic-arm-selector\n\n# Add your files\ncp /path/to/RoboticArm.py .\ncp /path/to/README.md .\ncp /path/to/requirements.txt .\n\n# Commit and push\ngit add .\ngit commit -m \"Add initial app files\"\ngit push\n</code></pre> <p>Once uploaded, the Space will build and deploy automatically. View and test your app via the Space URL.</p>"},{"location":"installation/#6-create-and-configure-the-hugging-face-dataset-xyz","title":"6. Create and Configure the Hugging Face Dataset <code>XYZ</code>","text":"<p>To store LLM result JSON files from the app, create a HF dataset repository. Name of the dataset can be anything! As long as you replace XYZ with your name of the dataset.</p>"},{"location":"installation/#a-create-the-dataset-repository","title":"A. Create the dataset repository","text":"<p>You can do this locally or via the website.</p> <ol> <li> <p>Via CLI (locally):    <pre><code>huggingface-cli login\nhuggingface-cli repo create XYZ --type dataset\n</code></pre>    Replace <code>XYZ</code> with your chosen name. Choose <code>public</code> or <code>private</code> as needed.</p> </li> <li> <p>Via Web UI:</p> <ul> <li>Go to New dataset.</li> <li>Name it <code>XYZ</code>, set visibility.</li> </ul> </li> <li> <p>Record the full repo ID: e.g. <code>your-username/XYZ</code>. This will be used in code.</p> </li> </ol>"},{"location":"installation/#b-add-hf-token-for-dataset-access","title":"B. Add HF token for dataset access","text":"<ul> <li>In your local environment (if testing outside Spaces), set:   <pre><code>export HF_TOKEN=\"&lt;your_hf_token&gt;\"\n</code></pre></li> <li>In the Space, you already added <code>HF_TOKEN</code> as a secret (Section 3).</li> </ul>"},{"location":"installation/#7-integrate-dataset-upload-in-roboticarmpy","title":"7. Integrate Dataset Upload in <code>RoboticArm.py</code>","text":"<p>Modify your Streamlit app to upload LLM results (<code>response_llm_json.json</code>) to the <code>XYZ</code> dataset.</p> <p>In order to divert to your dataset, locate the following code snippet and edit it appropriately to match your own Hugging Face dataset.</p> <pre><code>api = HfApi()\nwith open(\"response_llm_json.json\", \"rb\") as fobj:\n   api.upload_file(\n         path_or_fileobj=fobj,\n         path_in_repo=\"response_llm_json.json\",\n         repo_id=\"your-username/XYZ\",\n         repo_type=\"dataset\",\n         commit_message=\"Upload generated file\",\n         token=os.getenv(\"HF_TOKEN\")\n   )\n</code></pre> <p>Replace <code>repo_id</code> with your dataset ID/ dataset name.</p>"},{"location":"installation/#8-testing-the-upload-flow","title":"8. Testing the Upload Flow","text":"<ol> <li>Deploy or run the app in your Space.</li> <li>In the UI, enter a valid prompt to trigger LLM call and JSON creation.</li> <li>Observe the Streamlit UI for success message.</li> <li>Visit your Hugging Face dataset page: https://huggingface.co/datasets/your-username/XYZ to see uploaded JSON files.</li> </ol>"},{"location":"installation/#9-appendix-example-workflow-summary","title":"9. Appendix: Example Workflow Summary","text":"<ol> <li>Create Space and add secrets (<code>HF_TOKEN</code>, optionally <code>OPENAI_API_KEY</code>).</li> <li>Create HF dataset named <code>XYZ</code>, note <code>your-username/XYZ</code>.</li> <li>Prepare files (<code>RoboticArm.py</code>, <code>requirements.txt</code>, <code>README.md</code>) with upload logic.</li> <li>Upload files to Space via UI or Git, then deploy.</li> <li>Test the app: trigger LLM, observe upload success in UI.</li> <li>Verify dataset contents on HF Hub.</li> </ol>"},{"location":"unity/","title":"XR Scenario Creator","text":""},{"location":"unity/#1-components-implemented","title":"1 Components Implemented","text":"<p>Two key components have been developed: - BusinessLogic component - HuggingFaceJsonFetcher component</p> <p>The <code>BusinessLogic</code> is designed to accommodate the virtual robotic arms used in your Unity scene. You can assign it via the Unity Inspector, as shown below:</p> <p> Example showing the BusinessLogic component fields in the Unity Inspector.</p>"},{"location":"unity/#2-businesslogic-component-fields-overview","title":"2 BusinessLogic Component Fields Overview","text":"<p>All fields listed below are customizable for flexibility across different environments and training goals.</p> Field Name Type Description <code>deviceAnimators</code> Animator[] Robotic Devices with Animators attached <code>fireInstances</code> GameObject[] Prefabs for fire effects <code>fireOffset</code> Vector3 Offset to place fire effects <code>alarmClip</code> AudioClip Sound to play during fire incident <code>fireAudioSource</code> AudioSource Audio source for fire alarms <code>malfunctioningClip</code> AudioClip Sound to play during malfunction incident <code>malfuntioningAudioSource</code> AudioSource Audio source for malfunction alerts"},{"location":"unity/#21-businesslogic-component-field-descriptions","title":"2.1 BusinessLogic Component Field Descriptions","text":""},{"location":"unity/#211-deviceanimators-animator","title":"2.1.1 <code>deviceAnimators</code> (Animator[])","text":"<p>A list of Animator components. Assign any GameObject (e.g., robotic arms) that contains an Animator. If no Animator is present, it will not work.</p>"},{"location":"unity/#212-fireinstances-gameobject","title":"2.1.2 <code>fireInstances</code> (GameObject[])","text":"<p>A pool of disabled fire prefabs placed in the scene. The script will activate one randomly during a fire event.</p>"},{"location":"unity/#213-fireoffset-vector3","title":"2.1.3 <code>fireOffset</code> (Vector3)","text":"<p>Offset where the fire prefab appears relative to the device. Default is <code>(0, 1, 0)</code>.</p>"},{"location":"unity/#214-alarmclip-audioclip","title":"2.1.4 <code>alarmClip</code> (AudioClip)","text":"<p>The sound that plays during a fire incident. Use a looping alarm or suitable alert.</p>"},{"location":"unity/#215-fireaudiosource-audiosource","title":"2.1.5 <code>fireAudioSource</code> (AudioSource)","text":"<p>Audio source used to play <code>alarmClip</code>.</p>"},{"location":"unity/#216-malfunctioningclip-audioclip","title":"2.1.6 <code>malfunctioningClip</code> (AudioClip)","text":"<p>Audio clip triggered when a malfunction event occurs.</p>"},{"location":"unity/#217-malfuntioningaudiosource-audiosource","title":"2.1.7 <code>malfuntioningAudioSource</code> (AudioSource)","text":"<p>Audio source used to play the <code>malfunctioningClip</code>.</p>"},{"location":"unity/#3-unity-integration-guide-huggingface-json-fetcher","title":"3 Unity Integration Guide \u2013 HuggingFace JSON Fetcher","text":"<p>This section explains how to use the <code>HuggingFaceJsonFetcher</code> Unity component alongside <code>BusinessLogic</code> to create automated, LLM-driven VR scenarios using structured JSON input. </p> <p>The trainer promets in the provided LLM, the desired scenario (fire, malfuncationing incident or even both) and the LLM return a selection of the selected robotic arm which will virtually be set on fire.  When the traineed start the VR application, the library retrieves the structured ouput of the LLM, which indicated the robotic devices to be set on fire, malfunction or in case of a combination (a selection of max. 2 robotic devices) of the incidents which coresponding robotic device suffers which disaster.</p> <p>An example structured output is shown here, presenting also the JSON structure of the ouput:</p> <pre><code>{\n  \"Robotic_Arm_Virtual_Fire\": 1,\n  \"Robotic_Arm_Malfunctioning\": 2,\n  \"Reason_of_selection\": \"Robotic Arm Number 1 is the closest to the base with a distance of 1.43, which aligns with the description of the virtual fire being 'close to the base.' Robotic Arm Number 2, at a distance of 7.79, is the farthest and matches the description of the arm 'moving erratically at a far distance,' indicating it is potentially malfunctioning.\"\n}\n</code></pre> <p>This component, retrieves the LLM structured output from a dataset repository, which must be created in HuggingFace (see the corresponding documentation). In order to successfuly retrieve this information, we have to correctly set the following fields, show in the table below:</p> Parameter Type Description repoId string Hugging Face dataset repository ID (e.g., username/dataset_name). filename string JSON file name inside the dataset. token string (Optional) Hugging Face access token. Required for private datasets. <p> Example showing the HuggingFaceJsonFetcher component fields in the Unity Inspector.</p> <p>Below we show how a fully configured BusinessLogic and HuggingFaceJsonFetcher component looks like.</p> <p> Example showing how both components can be configured in the Unity Inspector. This is a fully functional configuration.</p>"},{"location":"unity/#31-huggingfacejsonfetcher-field-descriptions","title":"3.1 HuggingFaceJsonFetcher Field Descriptions","text":""},{"location":"unity/#311-repoid-string","title":"3.1.1 <code>repoId</code> (string)","text":"<p>The repoId is the name of the created dataset in Hugging Face (this is also the name that must be used in the LLM module in HugginFace). No links or any url of any kind is needed, only the profile name followed with the dataset name e.g. my-user-name/dataset-name. Inside this repository there must be at least one file (default can be the file with name response_llm_json.json ) in which the LLM will write its structured output.</p>"},{"location":"unity/#312-filename-string","title":"3.1.2 <code>filename</code> (string)","text":"<p>The name of the file the LLM writes the structured output in the designated dataset (see above field).</p>"},{"location":"unity/#313-token-string","title":"3.1.3 <code>token</code> (string)","text":"<p>In order to access the Hugging Face dataset, we need to create a token from the Hugging Face user account in order for the library to be authenticated and authorized to access the file mentioned above (same logic as in github, gitlab, etc). Refer to this link for additional information on this topic. Be aware to set the correct priviledges, otherwise the library will now be able to access the file, resulting in a 401 error response.</p> <p>BE CAREFULL not to publish or accidentally share the generated token. Line other tokens, is must not be publically available!</p>"}]}